{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Inference & Evaluation on Kaggle Dataset\n!pip install transformers datasets scikit-learn sentencepiece pandas\n\nimport os\nimport zipfile\nimport pandas as pd\nimport ast\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom tqdm import tqdm\n\n# --- 1. SETUP MODEL ---\nzip_path = \"/kaggle/input/finstructabsa/final_fin_model.zip\" # Ensure you uploaded this file!\nextract_path = \"my_trained_model\"\n\nif not os.path.exists(extract_path):\n    print(f\"Extracting {zip_path}...\")\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_path)\n\nmodel_dir = None\nfor root, dirs, files in os.walk(extract_path):\n    if \"config.json\" in files:\n        model_dir = root\n        break\nif model_dir is None: raise ValueError(\"Model not found in zip!\")\n\nprint(f\"‚úÖ Model loaded from: {model_dir}\")\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- 2. LOAD DATA FROM KAGGLE INPUT (The Modification) ---\nprint(\"Search for dataset...\")\ndataset_dir = \"/kaggle/input/aspect-based-sentiment-analysis-for-financial-news\"\ncsv_path = None\nfor root, dirs, files in os.walk(dataset_dir):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_path = os.path.join(root, file)\n            break\n\nif not csv_path: raise FileNotFoundError(\"Could not find CSV in /kaggle/input/...\")\nprint(f\"Reading data from: {csv_path}\")\n\ndf = pd.read_csv(csv_path)\n\n# --- 3. PROCESS DATA (Match Training Logic) ---\n# We need to extract (Sentence, Aspect, Sentiment) triplets\neval_rows = []\n\n# Choose 'test' or 'val'. Usually 'test' is for final report.\ntarget_split = 'test' \nprint(f\"Filtering for split: {target_split}\")\n\nfor _, row in df.iterrows():\n    # 1. Check Split\n    if str(row['split']).strip().lower() != target_split:\n        continue\n        \n    # 2. Parse Decisions (e.g., \"{'Stocks': 'Positive'}\")\n    try:\n        raw_decisions = row['Decisions']\n        # Fix common CSV quoting issues\n        if isinstance(raw_decisions, str):\n            if '\"\"' in raw_decisions: raw_decisions = raw_decisions.replace('\"\"', '\"')\n            decisions = ast.literal_eval(raw_decisions)\n        else:\n            decisions = raw_decisions\n            \n        if not isinstance(decisions, dict): continue\n\n        # 3. Create One Row per Aspect (ATSC Task)\n        for aspect, sentiment in decisions.items():\n            eval_rows.append({\n                'raw_text': row['Title'],\n                'term': aspect,\n                'labels': sentiment.lower()\n            })\n    except:\n        continue\n\ndf_eval = pd.DataFrame(eval_rows)\nprint(f\"Loaded {len(df_eval)} examples for evaluation.\")\n\n# --- 4. FORMAT PROMPTS (Crucial: Must Match Training) ---\n# We used InstructABSA-2 (ATSC) format in training\n# Prompt: Definition + 1 Example + Input\nprompt_prefix = \"\"\"Definition: Analyst task. Classify the sentiment of the financial Aspect as Positive, Negative, or Neutral.\nExample 1:\ninput: Revenue grew significantly in the last quarter. The aspect is Revenue.\noutput: positive\n\ninput: \"\"\"\n\ninputs = []\nfor _, row in df_eval.iterrows():\n    # Format: \"input: {text} The aspect is {term}\\noutput:\"\n    text_input = f\"{prompt_prefix}{row['raw_text']} The aspect is {row['term']}\\noutput:\"\n    inputs.append(text_input)\n\n# --- 5. RUN INFERENCE ---\nprint(\"Running predictions (This may take a few minutes)...\")\nbatch_size = 32\npredictions = []\n\nfor i in tqdm(range(0, len(inputs), batch_size)):\n    batch_texts = inputs[i : i + batch_size]\n    batch_inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(**batch_inputs, max_new_tokens=10)\n    \n    batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    predictions.extend(batch_preds)\n\n# --- 6. FINAL REPORT ---\ny_true = [str(l).lower().strip() for l in df_eval['labels']]\ny_pred = [str(p).lower().strip().replace('.','') for p in predictions]\n# Clean up any hallucinations (rare with T5-Large but possible)\nvalid_labels = ['positive', 'negative', 'neutral']\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"CLASSIFICATION REPORT ({target_split.upper()} SET)\")\nprint(\"=\"*50)\nprint(classification_report(y_true, y_pred, labels=valid_labels, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:29:32.424824Z","iopub.execute_input":"2025-12-28T14:29:32.425033Z","iopub.status.idle":"2025-12-28T14:31:32.827070Z","shell.execute_reply.started":"2025-12-28T14:29:32.425014Z","shell.execute_reply":"2025-12-28T14:31:32.826196Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-22.0.0\nExtracting /kaggle/input/finstructabsa/final_fin_model.zip...\n‚úÖ Model loaded from: my_trained_model/sentfin_model_large/atsc/googleflan-t5-large-fin_optimized_v3\n","output_type":"stream"},{"name":"stderr","text":"2025-12-28 14:30:36.515216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766932236.687581      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766932236.737576      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Search for dataset...\nReading data from: /kaggle/input/aspect-based-sentiment-analysis-for-financial-news/SEntFiN-v1.1_with_split.csv\nFiltering for split: test\nLoaded 3000 examples for evaluation.\nRunning predictions (This may take a few minutes)...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:36<00:00,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nCLASSIFICATION REPORT (TEST SET)\n==================================================\n              precision    recall  f1-score   support\n\n    positive     0.9073    0.9192    0.9132      1065\n    negative     0.8921    0.9141    0.9030       850\n     neutral     0.8914    0.8627    0.8768      1085\n\n    accuracy                         0.8973      3000\n   macro avg     0.8969    0.8987    0.8977      3000\nweighted avg     0.8973    0.8973    0.8972      3000\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell: Evaluation with Fixed Config for Unrecognized Model\nimport json\nimport os\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Config, AutoTokenizer\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nimport ast\nfrom tqdm import tqdm\n\n# --- 1. ROBUST MODEL LOADING ---\ntarget_model_path = \"/kaggle/input/m/joemum/finstructabsa/pytorch/default/1\"\n\nprint(f\"üîÑ Attempting to load model from: {target_model_path}\")\n\n# Step A: Find the actual config file\nconfig_path = None\nmodel_root = target_model_path\nfor root, dirs, files in os.walk(target_model_path):\n    if \"config.json\" in files:\n        config_path = os.path.join(root, \"config.json\")\n        model_root = root\n        break\n\nif not config_path:\n    raise ValueError(\"Could not find config.json in the input path!\")\n\nprint(f\"‚úÖ Found config at: {config_path}\")\n\n# Step B: Load Config & Force 't5' type\n# We load the JSON manually to inject the missing key\nwith open(config_path, 'r') as f:\n    config_dict = json.load(f)\n\n# FIX: Force the model type if missing\nif 'model_type' not in config_dict:\n    print(\"üõ†Ô∏è  Patching config: Adding 'model_type': 't5'\")\n    config_dict['model_type'] = 't5'\n    # Also ensure architectures list is correct if missing\n    if 'architectures' not in config_dict:\n        config_dict['architectures'] = [\"T5ForConditionalGeneration\"]\n\n# Create a config object from the dictionary\nconfig = T5Config.from_dict(config_dict)\n\n# Step C: Load Model using the specific T5 class and patched config\ntry:\n    # We use T5ForConditionalGeneration directly instead of AutoModel\n    model = T5ForConditionalGeneration.from_pretrained(model_root, config=config)\n    tokenizer = AutoTokenizer.from_pretrained(model_root)\n    \n    # Move to GPU\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = model.to(device)\n    print(f\"‚úÖ Success! Model loaded on {device}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Failed to load model: {e}\")\n    raise e\n\n# --- 2. LOAD DATA (Same as before) ---\ndataset_dir = \"/kaggle/input/aspect-based-sentiment-analysis-for-financial-news\"\ncsv_path = None\nfor root, dirs, files in os.walk(dataset_dir):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_path = os.path.join(root, file)\n            break\nif not csv_path: raise FileNotFoundError(\"CSV not found.\")\n\ndf = pd.read_csv(csv_path)\neval_rows = []\ntarget_split = 'test'\n\nfor _, row in df.iterrows():\n    if str(row['split']).strip().lower() != target_split: continue\n    try:\n        raw_decisions = row['Decisions']\n        if isinstance(raw_decisions, str):\n            if '\"\"' in raw_decisions: raw_decisions = raw_decisions.replace('\"\"', '\"')\n            decisions = ast.literal_eval(raw_decisions)\n        else:\n            decisions = raw_decisions\n        if isinstance(decisions, dict):\n            for aspect, sentiment in decisions.items():\n                eval_rows.append({'raw_text': row['Title'], 'term': aspect, 'labels': sentiment.lower()})\n    except: continue\n\ndf_eval = pd.DataFrame(eval_rows)\nprint(f\"Loaded {len(df_eval)} examples.\")\n\n# --- 3. RUN INFERENCE ---\n# Use the Standard InstructABSA Prompt\nprompt_prefix = \"\"\"Definition: Analyst task. Classify the sentiment of the financial Aspect as Positive, Negative, or Neutral.\nExample 1:\ninput: Revenue grew significantly in the last quarter. The aspect is Revenue.\noutput: positive\n\ninput: \"\"\"\n\ninputs = [f\"{prompt_prefix}{row['raw_text']} The aspect is {row['term']}\\noutput:\" for _, row in df_eval.iterrows()]\n\nprint(\"Running predictions...\")\nbatch_size = 64\npredictions = []\n\nfor i in tqdm(range(0, len(inputs), batch_size)):\n    batch_texts = inputs[i : i + batch_size]\n    batch_inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**batch_inputs, max_new_tokens=10)\n    predictions.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# --- 4. REPORT ---\ny_true = [str(l).lower().strip() for l in df_eval['labels']]\ny_pred = [str(p).lower().strip().replace('.','') for p in predictions]\nprint(classification_report(y_true, y_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:47:18.949351Z","iopub.execute_input":"2025-12-28T14:47:18.950058Z","iopub.status.idle":"2025-12-28T14:47:40.139605Z","shell.execute_reply.started":"2025-12-28T14:47:18.950034Z","shell.execute_reply":"2025-12-28T14:47:40.138713Z"}},"outputs":[{"name":"stdout","text":"üîÑ Attempting to load model from: /kaggle/input/m/joemum/finstructabsa/pytorch/default/1\n‚úÖ Found config at: /kaggle/input/m/joemum/finstructabsa/pytorch/default/1/sentfin_model_output/atsc/googleflan-t5-base-run1/config.json\n‚úÖ Success! Model loaded on cuda\nLoaded 3000 examples.\nRunning predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:10<00:00,  4.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       mixed     0.0000    0.0000    0.0000         0\n    negative     0.8613    0.9353    0.8968       850\n     neutral     0.9229    0.7945    0.8539      1085\n    positive     0.8660    0.9286    0.8962      1065\n\n    accuracy                         0.8820      3000\n   macro avg     0.6626    0.6646    0.6617      3000\nweighted avg     0.8853    0.8820    0.8811      3000\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":6}]}