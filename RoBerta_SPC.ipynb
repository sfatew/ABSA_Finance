{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14190488,"sourceType":"datasetVersion","datasetId":9027413},{"sourceId":14240814,"sourceType":"datasetVersion","datasetId":9085528}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Dataset Glance**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ast\nimport re\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW  # Imported from torch instead of transformers\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    get_linear_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\nfrom tqdm.auto import tqdm\nimport os\n\n# Strict Reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:42:54.365529Z","iopub.execute_input":"2025-12-26T14:42:54.366515Z","iopub.status.idle":"2025-12-26T14:42:54.376531Z","shell.execute_reply.started":"2025-12-26T14:42:54.366466Z","shell.execute_reply":"2025-12-26T14:42:54.375774Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def preprocess_sentfin_methodology(df):\n    processed_rows = []\n    \n    # Parse the dictionary string in 'Decisions' column\n    df['Decisions'] = df['Decisions'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n    \n    for idx, row in df.iterrows():\n        original_headline = row['Title']\n        decisions = row['Decisions']\n        all_entities = list(decisions.keys())\n        \n        for target_entity, sentiment in decisions.items():\n            # Label mapping: Positive: 0, Negative: 1, Neutral: 2\n            label = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}[sentiment]\n            \n            # Masking Logic\n            masked_text = original_headline\n            \n            # 1. Replace Other entities first\n            for ent in all_entities:\n                if ent != target_entity:\n                    masked_text = re.sub(rf'\\b{re.escape(ent)}\\b', 'Other', masked_text)\n            \n            # 2. Replace the Target entity\n            masked_text = re.sub(rf'\\b{re.escape(target_entity)}\\b', 'Target', masked_text)\n            \n            # 3. Clean special characters (Paper section 3.2.1)\n            masked_text = re.sub(r'[^\\w\\s]', '', masked_text)\n            \n            processed_rows.append({\n                'sentence': masked_text,\n                'label': label,\n                'split': row['split']\n            })\n            \n    return pd.DataFrame(processed_rows)\n\n# Load raw data\nraw_df = pd.read_csv('/kaggle/input/aspect-based-sentiment-analysis-for-financial-news/SEntFiN-v1.1_with_split.csv')\ndf_processed = preprocess_sentfin_methodology(raw_df)\n\n# Splits\ntrain_full = df_processed[df_processed['split'] == 'train'].reset_index(drop=True)\ntest_df = df_processed[df_processed['split'] == 'test'].reset_index(drop=True)\n\n# 10% Validation split from train\ntrain_df, val_df = train_test_split(train_full, test_size=0.1, random_state=SEED, stratify=train_full['label'])\n\nprint(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:45:23.069975Z","iopub.execute_input":"2025-12-26T14:45:23.070774Z","iopub.status.idle":"2025-12-26T14:45:24.260368Z","shell.execute_reply.started":"2025-12-26T14:45:23.070738Z","shell.execute_reply":"2025-12-26T14:45:24.259433Z"}},"outputs":[{"name":"stdout","text":"Train: 9255 | Val: 1029 | Test: 3000\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class SEntFiNDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=30):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        encoding = self.tokenizer.encode_plus(\n            row['sentence'],\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(row['label'], dtype=torch.long)\n        }\n\n# THE PAPER'S SPECIFIC CHECKPOINT FOR ROBERTA (B)\nMODEL_NAME = \"openai-community/roberta-base-openai-detector\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntrain_loader = DataLoader(SEntFiNDataset(train_df, tokenizer), batch_size=64, shuffle=True)\nval_loader = DataLoader(SEntFiNDataset(val_df, tokenizer), batch_size=64)\ntest_loader = DataLoader(SEntFiNDataset(test_df, tokenizer), batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:45:39.20822Z","iopub.execute_input":"2025-12-26T14:45:39.208794Z","iopub.status.idle":"2025-12-26T14:45:40.289436Z","shell.execute_reply.started":"2025-12-26T14:45:39.208764Z","shell.execute_reply":"2025-12-26T14:45:40.288774Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805b4749194c427e90ff9477caaf66b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6698e3eec543228f066564bfc483f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f720e58ebeb44bb8eb267b58bf00f91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9468b1f966d149cbb895037e72cc5e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c15035f5d54ad7aab2843a51f203e7"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=3,\n    hidden_dropout_prob=0.2,\n    attention_probs_dropout_prob=0.2,\n    ignore_mismatched_sizes=True \n)\nmodel.to(DEVICE)\n\n# Weight Decay 0.01 per RoBERTa (B) configuration\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\ntotal_steps = len(train_loader) * 10\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\nbest_acc = 0\nfor epoch in range(10):\n    model.train()\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n        input_ids, mask, labels = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['labels'].to(DEVICE)\n        outputs = model(input_ids, attention_mask=mask, labels=labels)\n        outputs.loss.backward()\n        optimizer.step()\n        scheduler.step()\n    \n    # Validation\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids, mask, labels = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['labels'].to(DEVICE)\n            preds = torch.argmax(model(input_ids, attention_mask=mask).logits, dim=1)\n            correct += (preds == labels).sum().item()\n    \n    val_acc = correct / len(val_df)\n    print(f\"Val Accuracy: {val_acc:.4f}\")\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), 'roberta_sentfin_best.pt')\n        print(\"Model Saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:45:53.088505Z","iopub.execute_input":"2025-12-26T14:45:53.089093Z","iopub.status.idle":"2025-12-26T14:54:14.149715Z","shell.execute_reply.started":"2025-12-26T14:45:53.089049Z","shell.execute_reply":"2025-12-26T14:54:14.149047Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25baab427f5e4c52b7903138902cf7e4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at openai-community/roberta-base-openai-detector and are newly initialized because the shapes did not match:\n- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e01b9736b347259d85ec687ef320a6"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.7862\nModel Saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b70b4e61684d8c9e332a7e3f94f63f"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8056\nModel Saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016760148929420a93689fd89368e0f4"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8309\nModel Saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5873ddd3e2a8431ebb3798c981a603ae"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8552\nModel Saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779c8060986946d8962bb1a462e9ad47"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8698\nModel Saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a42331766f4f529e4b8b844dd34618"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8698\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"273a4e6a9b5e4baea830e515b42acb0b"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8814\nModel Saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96eb9069554d486993959b8e4114ec6c"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8795\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe8e8ee26ed4f0282e3e9c450bd52e6"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8776\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11949fcc0dbc4b09b133d1ff25b836c1"}},"metadata":{}},{"name":"stdout","text":"Val Accuracy: 0.8785\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load the Best Model Weights\nmodel.load_state_dict(torch.load('roberta_sentfin_best.pt'))\nmodel.to(DEVICE)\nmodel.eval()\n\ntest_preds, test_labels = [], []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n        input_ids, mask, labels = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['labels'].to(DEVICE)\n        outputs = model(input_ids=input_ids, attention_mask=mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        test_preds.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntarget_names = ['Positive', 'Negative', 'Neutral']\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS: RoBERTa (B) on SEntFiN Test Set\")\nprint(\"=\"*60)\n\nprint(f\"Overall Accuracy: {accuracy_score(test_labels, test_preds)*100:.2f}%\")\nprint(f\"Overall F1-Score: {f1_score(test_labels, test_preds, average='weighted')*100:.2f}%\")\nprint(\"-\" * 60)\n\nreport = classification_report(test_labels, test_preds, target_names=target_names, digits=4, output_dict=True)\nprint(f\"{'Class':<12} | {'Accuracy (%)':<15} | {'F1-Score (%)':<15}\")\nprint(\"-\" * 48)\n\nfor label in target_names:\n    acc = report[label]['recall'] * 100\n    f1 = report[label]['f1-score'] * 100\n    print(f\"{label:<12} | {acc:<15.2f} | {f1:<15.2f}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:55:21.337419Z","iopub.execute_input":"2025-12-26T14:55:21.338133Z","iopub.status.idle":"2025-12-26T14:55:26.817393Z","shell.execute_reply.started":"2025-12-26T14:55:21.338099Z","shell.execute_reply":"2025-12-26T14:55:26.81663Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ea4ce865254078acceed3562ab0723"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nFINAL RESULTS: RoBERTa (B) on SEntFiN Test Set\n============================================================\nOverall Accuracy: 86.43%\nOverall F1-Score: 86.38%\n------------------------------------------------------------\nClass        | Accuracy (%)    | F1-Score (%)   \n------------------------------------------------\nPositive     | 88.92           | 88.22          \nNegative     | 90.71           | 88.01          \nNeutral      | 80.65           | 83.29          \n============================================================\n","output_type":"stream"}],"execution_count":15}]}